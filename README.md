# End-to-End-StreamingData-AWS-Kinesis-

In this project trying to  take streaming logs files generated by python code and make it throw a 
    Amazon Kinesis Firehose >> Amazon Kinesis Data Analytics >> second Amazon Kinesis Firehose >> Amazon OpenSearch
- As in this architecture:

![image](https://user-images.githubusercontent.com/55424201/201802528-ab5821b0-e967-4d84-b143-f26798cb6f41.png)


so :

 - first created  an EC2 Instance and configured the IAM roles to give access to AmazonKinesisFirehose 
 - connected to the machine with ssh connect 
 - install some dependencies on my machine with the command line 
 - clone a GitHub repo called Fake-Apache-Log-Generator and the link of it is: https://github.com/kiritbasu/Fake-Apache-Log-Generator
   with this repo, we can generate streaming log files as our data 
 
 - Created an Amazon Kinesis Data Firehose Delivery and made the source as Direct PUT and the destination S3 bucket
 
 ![image](https://user-images.githubusercontent.com/55424201/201809978-0a6989dd-5d7b-496e-a202-6b521ad3757d.png)
 
 - Installed the Amazon Kinesis Agent on the EC2 Instance by command line
 
 - modify the agent configuration file to capture all the data from a specific directory (output directory of the python file in the repo) which will convert 
   the logs format into JSON format 
 
 - go to the agent directory and run the python file in the repo multiple times to get streaming logs(data)
 - Started the agent manually by issuing a command line 
 - the streaming data now will be in the s3 bucket and converted to JSON format
 - can't create the Kinesis Data Analytics now because the destination of it will be Kinesis Data Firehose and can't create Kinesis Data Firehose 
   because the destination of it will be Amazon OpenSearch so :
 - created the Amazon OpenSearch  
 - created the second Kinesis Data Firehose and made the  source Direct PUT and the destination OpenSearch and made an index for the data
 - created  an Amazon Kinesis Data Analytics Application :
    - Connected to streaming data which will be the first Data Firehose and discovered the schema. 
    - created STREAM and a PUMP with SQL 
    - connected to the destination which will be the second Data Firehose
    
 - finally Viewed the Aggregated Streaming Data by OpenSearch and made a simple  graph and some search 
 - you can watch this video with all the steps here : https://www.linkedin.com/posts/mohamed-abohassan-6509641a9_dataengineering-python-dataanalytics-activity-6998262573120839681-6-Cx?utm_source=share&utm_medium=member_desktop
 
 
